---
title: "Title"
author:
  - Joann Jun
  - Lily Fransen
  - Karim Zaggoti
date: "July 25, 2025"
toc: true
format:
  html:
    theme: cosmo
    html-math-method: katex
    self-contained: true
execute:
  echo: false
  warning: false
  message: false
---

---

## Introduction 

In recent years, the mental health epidemic has become a growing public health concern. Factors such as social isolation, economic uncertainty, the stigmatization of mental illness, and limited access to care have all contributed to an increase in reported cases of anxiety, depression, and other mental health conditions. According to the Centers for Disease Control and Prevention (CDC), more than one in five adults in the United States live with a mental illness (insert citation). Poor mental health can significantly affect an individual’s quality of life, impacting emotional well-being, job performance, relationships, and physical health. Yet, according to the National Institutes of Health (NIH), only about half of those experiencing mental illness receive treatment (insert citation).

This concerning gap between mental health needs and treatment raises an important question: does access to mental health care influence mental health outcomes?

One key component in addressing this issue is ensuring that individuals have access to qualified mental health professionals. However, access to care is not evenly distributed across geographic regions. While some counties benefit from a dense network of providers, others face critical shortages, leaving residents without adequate support or timely treatment.

This study aims to explore the relationship between the availability of mental health professionals and the mental well being of a population. Specifically, we ask: Does the number of mental health professionals per county affect the average number of poor mental health days? Understanding this relationship can help inform policy decisions, guide resource allocation, and identify areas most in need of improved mental health infrastructure, ultimately working toward better mental health outcomes at the community level.

## Data

Describe the data you’re using in detail, where you accessed it, along with relevant exploratory data analysis (EDA). You should also include descriptions of any relevant data pre-processing steps (e.g., whether you consider specific observations, create any meaningful features, etc.---but don't mention minor steps like column type conversion, filtering out unnecessary rows)

We are using the 2025 County Health Rankings Dataset, which is collected by the University of Wisconsin Population Health Institute. The data ranks each county in all 50 states based on their health outcomes and variety of health factors. This dataset is widely used by policymakers and researchers to better understand and address factors influencing community and national health. Each row represents one U.S. county and its county-level metrics, and each column is a particular variable. We looked at 6 specific columns, focusing only on the raw values in that category. These were: mental health providers, poor mental health days, lack of social and emotional support, suicide rate, frequent mental distress, and county FIPS code. To standardize for population, we also engineered a variable of the proportion of Mental Health Providers per 100,000 people and used this to improve our visualizations.

--ADD EDA--

We saw that the data was normally distributed.



## Methods

Describe the modeling techniques you chose, their assumptions, justifications for why they are appropriate for the problem, and how you're comparing/evaluating the different methods.

Poisson (add equation)

$log(\mu_i) = \sum_{j=0}^{M}x_{ij}\beta_j$ $j=1...6$

- check ass for poisson
- Normality: shapiro wilk -- p-value is ___ which is less than the significance level of $\alpha$ = 0.05. This means that the normality assumption is satisfied. Variance
To test which variables were significant, we used Poisson regression. We chose this
- Indepependence
- Equal mean and variance

ML
Since we had a large sample, we decided to implement some machine learning to create predictive models. We selected Random Forest and XGBoost and evaluated them use RMSE. The lower the RMSE is the better the model is. 

## Results

Describe your results. This can include tables and plots showing your results, as well as text describing how your models worked and the appropriate interpretations of the relevant output. (Note: Don’t just write out the textbook interpretations of all model coefficients. Instead, interpret the output that is relevant for your question of interest that is framed in the introduction)

-- ADD PLOTS IN THIS SECTION --

mention this
 - There was no significant relationship between proportion/ratio of population to mental health providers
 - not a not enough mental health providers problem --> implies it could be a social problem since lack of social and emotional support and feelings of loneliness also top predictors

When evaluating the 2 models, Random Forest and XGBoost, we saw that the RMSE value for XGBoost was 0.44317 which was slightly lower than the RMSE for Random Forest, which was _____. This meant that the XGBoost model has a greater accuracy and predictive power compared to Random Forest.

We also used SHAP to see feature importance in predicting the amount of poor mental health days in the model. We saw that 

add SHAP and % RMSE 

RMSE -- SHAP & Feature Importance


## Recommendations

Make recommendations to UHG/Optum on specific targeted programs that can address the question of interest. Use evidence found in your analysis to back up your recommendations. This should be aimed at a non-technical reader.

As we mentioned before, there was no significant relationship between the ratio of mental health providers in a county and the amount of poor mental health days. This means that the number of mental health providers in a county does not matter and that the reason for these poor mental health days because of social reasons. 

We would recommend doing some more social outreach.


## Discussion

Give your conclusions and summarize what you have learned with regards to your question of interest. Are there any limitations with the approaches you used? What do you think are the next steps to follow-up your project?


-- SUMMARY OF RESEARCH -- 
- brief overview of everything we found out


--LIMITATIONS--
- more data (geographic like terrain,Distance to mental health provider, Transportation)
Look into social (race, gender) and economic factors (income, insurance)
maybe online mental health providerrs, internet access (online therapy)
Data on quality of therapy/mental healthcare

## Appendix: A quick tutorial

## Sources

**(Feel free to remove this section when you submit)**

This a Quarto document. 
To learn more about Quarto see <https://quarto.org>.
You can use the Render button to see what it looks like in HTML.

### Text formatting

Text can be bolded with **double asterisks** and italicized with *single asterisks*. 
Monospace text, such as for short code snippets, uses `backticks`.
(Note these are different from quotation marks or apostrophes.) Links are
written [like this](http://example.com/).

Bulleted lists can be written with asterisks:

* Each item starts on a new line with an asterisk.
* Items should start on the beginning of the line.
* Leave blank lines after the end of the list so the list does not continue.

Mathematics can be written with LaTeX syntax using dollar signs. 
For instance, using single dollar signs we can write inline math: $(-b \pm \sqrt{b^2 - 4ac})/2a$.

To write math in "display style", i.e. displayed on its own line centered on the
page, we use double dollar signs:
$$
x^2 + y^2 = 1
$$


### Code blocks

Code blocks are evaluated sequentially when you hit Render. 
As the code runs, `R` prints out which block is running, so naming blocks is useful if you want to know which one takes a long time. 
After the block name, you can specify [chunk options](https://yihui.org/knitr/options/). 
For example, `echo` controls whether the code is printed in the document. 
By default, output is printed in the document in monospace:

```{r, echo = FALSE}
head(mtcars)
```

Chunk options can also be written inside the code block, which is helpful for really long options, as we'll see soon.

```{r}
#| echo: false
head(mtcars)
```

### Figures

If a code block produces a plot or figure, this figure will automatically be inserted inline in the report. That is, it will be inserted exactly where the code block is.

```{r}
#| fig-width: 5
#| fig-height: 3.5
#| fig-cap: "This is a caption. It should explain what's in the figure and what's interesting about it. For instance: There is a negative, strong linear correlation between miles per gallon and horsepower for US cars in the 1970s."

library(tidyverse)
mtcars |> 
  ggplot(aes(x = mpg, y = hp)) +
  geom_point() +
  labs(x = "Miles per gallon",
       y = "Horsepower")
```

Notice the use of `fig-width` and `fig-height` to control the figure's size (in inches). 
These control the sizes given to `R` when it generates the plot, so `R` proportionally adjusts the font sizes to be large enough.

### Tables

Use the `knitr::kable()` function to print tables as HTML:

```{r}
mtcars |> 
  slice(1:5) |> 
  knitr::kable()
```

We can summarize model results with a table. 
For instance, suppose we fit a linear regression model:

```{r}
#| echo: true
model1 <- lm(mpg ~ disp + hp + drat, data = mtcars)
```

It is *not* appropriate to simply print `summary(model1)` into the report. 
If we want the reader to understand what models we have fit and what their results are, we should provide a nicely formatted table. 
A simple option is to use the `tidy()` function from the `broom` package to get a data frame of the model fit, and simply report that as a table.

```{r }
#| results: "asis"
#| tbl-cap: "Predicting fuel economy using vehicle features."
library(broom)
model1 |> 
  tidy() |>
  knitr::kable(digits = 2,
               col.names = c("Term", "Estimate", "SE", "t", "p"))
```
