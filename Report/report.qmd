---
title: "Title"
author:
  - Joann Jun
  - Lily Fransen
  - Karim Zaggoti
date: "July 25, 2025"
toc: true
format:
  html:
    theme: cosmo
    html-math-method: katex
    self-contained: true
execute:
  echo: false
  warning: false
  message: false
---

---

## Introduction 

In recent years, the mental health epidemic has become a growing public health concern. Factors such as social isolation, economic uncertainty, the stigmatization of mental illness, and limited access to care have all contributed to an increase in reported cases of anxiety, depression, and other mental health conditions. According to the Centers for Disease Control and Prevention (CDC), more than one in five adults in the United States live with a mental illness (insert citation). Poor mental health can significantly affect an individual’s quality of life, impacting emotional well-being, job performance, relationships, and physical health. Yet, according to the National Institutes of Health (NIH), only about half of those experiencing mental illness receive treatment (insert citation).

This concerning gap between mental health needs and treatment raises an important question: does access to mental health care influence mental health outcomes?

One key component in addressing this issue is ensuring that individuals have access to qualified mental health professionals. However, access to care is not evenly distributed across geographic regions. While some counties benefit from a dense network of providers, others face critical shortages, leaving residents without adequate support or timely treatment.

This study aims to explore the relationship between the availability of mental health professionals and the mental well being of a population. Specifically, we ask: Does the number of mental health professionals per county affect the average number of poor mental health days? Understanding this relationship can help inform policy decisions, guide resource allocation, and identify areas most in need of improved mental health infrastructure, ultimately working toward better mental health outcomes at the community level.

## Data

Describe the data you’re using in detail, where you accessed it, along with relevant exploratory data analysis (EDA). You should also include descriptions of any relevant data pre-processing steps (e.g., whether you consider specific observations, create any meaningful features, etc.---but don't mention minor steps like column type conversion, filtering out unnecessary rows)

We are using the 2025 County Health Rankings Dataset, which is collected by the University of Wisconsin Population Health Institute. The data ranks each county in all 50 states based on their health outcomes and variety of health factors. This dataset is widely used by policymakers and researchers to better understand and address factors influencing community and national health. Each row represents one U.S. county and its county-level metrics, and each column is a particular variable. We looked at 6 specific columns, focusing only on the raw values in that category. These were: mental health providers, poor mental health days, lack of social and emotional support, suicide rate, frequent mental distress, and county FIPS code. To standardize for population, we also engineered a variable of the proportion of Mental Health Providers per 100,000 people and used this to improve our visualizations.

As part of our exploratory data analysis, we examined the relationship between provider access and suicide rates. While the overall trend shows only a slight association between these two variables, a few striking outliers emerge (primarily Alaskan counties) with both high suicide rates and high provider availability. This pattern suggests that some communities may have responded to elevated suicide risks by increasing access to mental health services as a preventative strategy, rather than the provider access itself being a direct deterrent of higher suicide rates.

We saw that the data was normally distributed.

```{r}
ggplot(mental_health_full, aes(x = provider_ratio, y = suicide_rate, color = social_support)) +
  geom_point(alpha = 0.6, size = 2) +
  scale_color_gradient(low = "skyblue", high = "darkred") +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
  geom_text_repel(
    data = mental_health_full |> filter(suicide_rate > 25),
    aes(label = county_name),
    size = 2.8,
    color = "gray30",
    max.overlaps = 15
  ) +
  labs(
    title = "As Provider Access Falls and Isolation Grows, Suicide Rates Rise",
    x = "Population per Mental Health Provider",
    y = "Suicide Rate (per 100,000)",
    color = "Lack of Social Support"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold"),
    plot.subtitle = element_text(margin = ggplot2::margin(t = 5, b = 10)), 
    legend.position = "right"
  )
```

## Methods

Describe the modeling techniques you chose, their assumptions, justifications for why they are appropriate for the problem, and how you're comparing/evaluating the different methods.

Poisson (add equation)

$log(\mu_i) = \sum_{j=0}^{M}x_{ij}\beta_j$ $j=1...6$

- check ass for poisson
- Normality: shapiro wilk -- p-value is ___ which is less than the significance level of $\alpha$ = 0.05. This means that the normality assumption is satisfied. Variance
- Indepependence
- Equal mean and variance

To test which variables were significant, we used Poisson regression. We chose this method because our outcome variable, poor mental health days, is count-based and non-negative. We checked assumptions for Poisson regression including independence and the equality of mean and variance. Due to observed overdispersion, we used a quasi-Poisson model to account for greater variance. All predictors were standardized before fitting the model. The Shapiro-Wilk test for residual normality yielded a p-value of (will add later), supporting the model's assumptions. The regression revealed that feelings of loneliness and lack of social support were the only statistically significant predictors. We visualized these results using a bar chart of percent change effects, which clearly highlighted the stronger social predictors.

```{r}
# Standardize predictors
standardized_data <- mental_health_full |>
  mutate(across(
    c(provider_ratio, loneliness, social_support, suicide_rate),
    ~ scale(.)[, 1]
  ))

# Fit Quasi-Poisson regression
quasi_model_selected <- glm(
  poor_mental_days ~ provider_ratio + loneliness + social_support + suicide_rate,
  data = standardized_data,
  family = quasipoisson(link = "log")
)

# Generate clean summary table with p-values
clean_summary <- broom::tidy(quasi_model_selected, conf.int = TRUE) |>
  filter(term != "(Intercept)") |>
  mutate(
    `Percent Change in Poor Mental Health Days` = 100 * (exp(estimate) - 1),
    `95% CI Lower` = 100 * (exp(conf.low) - 1),
    `95% CI Upper` = 100 * (exp(conf.high) - 1),
    Predictor = case_when(
      term == "provider_ratio" ~ "Provider Ratio",
      term == "loneliness" ~ "Loneliness (0–1 Scale)",
      term == "social_support" ~ "Lack of Social Support (0–1 Scale)",
      term == "suicide_rate" ~ "Suicide Rate"
    )
  ) |>
  select(
    Predictor,
    `Percent Change in Poor Mental Health Days`,
    `95% CI Lower`,
    `95% CI Upper`,
    `p.value`
  )

# Display the table
print(clean_summary, n = Inf, width = Inf)

```

ML
Since we had a large sample, we decided to implement some machine learning to create predictive models. We selected Random Forest and XGBoost and evaluated them use RMSE. The lower the RMSE is the better the model is. 


```{r}
# Prepare data for modeling
rf_data <- mental_health_full |>
  select(
    poor_mental_days, social_support, loneliness,
    suicide_rate, crude_suicide, provider_ratio, population
  )

# Split into training and testing sets
set.seed(42)
train_indices <- createDataPartition(rf_data$poor_mental_days, p = 0.8, list = FALSE)
train_data <- rf_data[train_indices, ]
test_data  <- rf_data[-train_indices, ]

train_data <- na.omit(train_data)
test_data <- na.omit(test_data)

# Train Random Forest model
rf_model <- randomForest(
  poor_mental_days ~ .,
  data = train_data,
  ntree = 500,
  importance = TRUE
)

# Evaluate model
predictions <- predict(rf_model, test_data)
mse <- mean((predictions - test_data$poor_mental_days)^2)
rmse <- sqrt(mse)

cat(" Mean Squared Error (MSE):", round(mse, 3), "\n")
cat(" Root Mean Squared Error (RMSE):", round(rmse, 3), "\n")

# Variable importance
var_imp <- importance(rf_model) |>
  as.data.frame() |>
  rownames_to_column("Variable") |>
  mutate(`%IncRMSE` = `%IncMSE` / 2) |>
  arrange(desc(`%IncRMSE`))

# Plot variable importance
ggplot(var_imp, aes(x = reorder(Variable, `%IncRMSE`), y = `%IncRMSE`)) +
  geom_col(fill = "blue") +
  coord_flip() +
  labs(
    title = "Variable Importance from Random Forest",
    x = "Predictor",
    y = "% Increase in RMSE"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title.y = element_text(margin = ggplot2::margin(r = 10)),
    axis.title.x = element_text(margin = ggplot2::margin(t = 10))
  )
```

## Results

Describe your results. This can include tables and plots showing your results, as well as text describing how your models worked and the appropriate interpretations of the relevant output. (Note: Don’t just write out the textbook interpretations of all model coefficients. Instead, interpret the output that is relevant for your question of interest that is framed in the introduction)

-- ADD PLOTS IN THIS SECTION --

mention this
 - There was no significant relationship between proportion/ratio of population to mental health providers
 - not a not enough mental health providers problem --> implies it could be a social problem since lack of social and emotional support and feelings of loneliness also top predictors

When evaluating the 2 models, Random Forest and XGBoost, we saw that the RMSE value for XGBoost was 0.44317 which was slightly lower than the RMSE for Random Forest, which was _____. This meant that the XGBoost model has a greater accuracy and predictive power compared to Random Forest.

We also used SHAP to see feature importance in predicting the amount of poor mental health days in the model. We saw that 

add SHAP and % RMSE 

RMSE -- SHAP & Feature Importance


## Recommendations

Make recommendations to UHG/Optum on specific targeted programs that can address the question of interest. Use evidence found in your analysis to back up your recommendations. This should be aimed at a non-technical reader.

As we mentioned before, there was no significant relationship between the ratio of mental health providers in a county and the amount of poor mental health days. This means that the number of mental health providers in a county does not matter and that the reason for these poor mental health days because of social reasons. 

We would recommend doing some more social outreach.


## Discussion

Give your conclusions and summarize what you have learned with regards to your question of interest. Are there any limitations with the approaches you used? What do you think are the next steps to follow-up your project?


-- SUMMARY OF RESEARCH -- 
Our analysis revealed that mental health provider density does not have a strong correlation with number of poor mental health days, challenging the common assumption that increasing the number of providers will lead to better mental health outcomes overall. This told us that there are other variables playing a bigger role here. Among the group tested, feelings of loneliness and lack of social and emotional support led as the strongest predictors of poor mental health days. Taking a further step with our findings, we also discovered an alarming pattern in Alaskan counties, pointing to disparities in mental health outcomes between their urban and rural regions. This new view reinforced the importance of the social determinants of health and helped direct our discussions and recommendations. When comparing predictive models, we found XGBoost to be a better predictive model, as it captued the non-linear relationships between all our variables better. 


--LIMITATIONS--
- more data (geographic like terrain,Distance to mental health provider, Transportation)
- Look into social (race, gender) and economic factors (income, insurance)
- maybe online mental health providerrs, internet access (online therapy)
- Data on quality of therapy/mental healthcare

--RECOMMENDATIONS--


## Appendix: A quick tutorial

## Sources

**(Feel free to remove this section when you submit)**

This a Quarto document. 
To learn more about Quarto see <https://quarto.org>.
You can use the Render button to see what it looks like in HTML.

### Text formatting

Text can be bolded with **double asterisks** and italicized with *single asterisks*. 
Monospace text, such as for short code snippets, uses `backticks`.
(Note these are different from quotation marks or apostrophes.) Links are
written [like this](http://example.com/).

Bulleted lists can be written with asterisks:

* Each item starts on a new line with an asterisk.
* Items should start on the beginning of the line.
* Leave blank lines after the end of the list so the list does not continue.

Mathematics can be written with LaTeX syntax using dollar signs. 
For instance, using single dollar signs we can write inline math: $(-b \pm \sqrt{b^2 - 4ac})/2a$.

To write math in "display style", i.e. displayed on its own line centered on the
page, we use double dollar signs:
$$
x^2 + y^2 = 1
$$


### Code blocks

Code blocks are evaluated sequentially when you hit Render. 
As the code runs, `R` prints out which block is running, so naming blocks is useful if you want to know which one takes a long time. 
After the block name, you can specify [chunk options](https://yihui.org/knitr/options/). 
For example, `echo` controls whether the code is printed in the document. 
By default, output is printed in the document in monospace:

```{r, echo = FALSE}
head(mtcars)
```

Chunk options can also be written inside the code block, which is helpful for really long options, as we'll see soon.

```{r}
#| echo: false
head(mtcars)
```

### Figures

If a code block produces a plot or figure, this figure will automatically be inserted inline in the report. That is, it will be inserted exactly where the code block is.

```{r}
#| fig-width: 5
#| fig-height: 3.5
#| fig-cap: "This is a caption. It should explain what's in the figure and what's interesting about it. For instance: There is a negative, strong linear correlation between miles per gallon and horsepower for US cars in the 1970s."

library(tidyverse)
mtcars |> 
  ggplot(aes(x = mpg, y = hp)) +
  geom_point() +
  labs(x = "Miles per gallon",
       y = "Horsepower")
```

Notice the use of `fig-width` and `fig-height` to control the figure's size (in inches). 
These control the sizes given to `R` when it generates the plot, so `R` proportionally adjusts the font sizes to be large enough.

### Tables

Use the `knitr::kable()` function to print tables as HTML:

```{r}
mtcars |> 
  slice(1:5) |> 
  knitr::kable()
```

We can summarize model results with a table. 
For instance, suppose we fit a linear regression model:

```{r}
#| echo: true
model1 <- lm(mpg ~ disp + hp + drat, data = mtcars)
```

It is *not* appropriate to simply print `summary(model1)` into the report. 
If we want the reader to understand what models we have fit and what their results are, we should provide a nicely formatted table. 
A simple option is to use the `tidy()` function from the `broom` package to get a data frame of the model fit, and simply report that as a table.

```{r }
#| results: "asis"
#| tbl-cap: "Predicting fuel economy using vehicle features."
library(broom)
model1 |> 
  tidy() |>
  knitr::kable(digits = 2,
               col.names = c("Term", "Estimate", "SE", "t", "p"))
```
